{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35538839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import gymnasium as gym\n",
    "from minigrid.wrappers import FlatObsWrapper\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import deque\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import datetime\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b92249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Hyper-paramètres et environnement ────────────────────────────────────────\n",
    "env_name = \"MiniGrid-Empty-8x8-v0\"\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.02\n",
    "epsilon_decay = 0.97\n",
    "gamma = 0.99\n",
    "batch_size= 32\n",
    "memory_size = 50_000\n",
    "episodes = 100\n",
    "target_update_freq = 10\n",
    "\n",
    "best_model_path = \"best_q_model.weights.h5\"\n",
    "\n",
    "# Wrapper\n",
    "env = FlatObsWrapper(gym.make(env_name))\n",
    "state_shape  = env.observation_space.shape[0]\n",
    "action_shape = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18446773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Fonction pour la création du modèle ────────────────────────────────────────────────────\n",
    "def create_q_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(state_shape,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(action_shape, activation='linear'),\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0003), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72986b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des deux modèles\n",
    "q_model      = create_q_model()\n",
    "target_model = create_q_model()\n",
    "\n",
    "target_model.set_weights(q_model.get_weights())\n",
    "\n",
    "memory = deque(maxlen=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b3dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_transition(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10be6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch():\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    state, action, reward, next_state, done = map(np.asarray, zip(*batch))\n",
    "    return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f98503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── TensorBoard  ──────────────────────────────────────────────────\n",
    "log_dir = \"logs/minigrid/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.7, patience=10, min_lr=0.00001)\n",
    "early_stop = EarlyStopping(monitor='loss', patience=30, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(filepath=best_model_path, monitor='loss', save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97967c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Politique epsilon-greedy ou la prise de décision ───────────────────────────────────────────────────────\n",
    "def epsilon_greedy_policy(state, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.choice(action_shape)\n",
    "    q_values = q_model.predict(state[np.newaxis], verbose=0)[0]\n",
    "    return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bd06cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Entraînement ─────────────────────────────────────────────────\n",
    "def train_step():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    state, action, reward, next_state, done = sample_batch()\n",
    "\n",
    "    next_q_values = target_model.predict(next_state, verbose=0)\n",
    "    max_next_q_values = np.max(next_q_values, axis=1)\n",
    "\n",
    "    target_q_values = q_model.predict(state, verbose=0)\n",
    "    for i, action in enumerate(action):\n",
    "        target_q_values[i][action] = reward[i] if done[i] else reward[i] + gamma * max_next_q_values[i]\n",
    "\n",
    "    q_model.fit(state, target_q_values, verbose=0, callbacks=[tensorboard, reduce_lr, early_stop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6827440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Boucle d’entraînement par épisode─────────────────────────────────────────────────────\n",
    "reward_history = []\n",
    "success_history = []\n",
    "loss_history = []\n",
    "epsilon_history = []\n",
    "\n",
    "for episode in range(1, episodes + 1):\n",
    "    state, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    success = False\n",
    "    step_count = 0\n",
    "\n",
    "    while not done:\n",
    "        action = epsilon_greedy_policy(state, epsilon)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        store_transition(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        step_count += 1\n",
    "\n",
    "        if reward > 0:\n",
    "            success = True\n",
    "        train_step()\n",
    "    # train_step()\n",
    "\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    \n",
    "    if episode % target_update_freq == 0:\n",
    "        target_model.set_weights(q_model.get_weights())\n",
    "\n",
    "    reward_history.append(episode_reward)\n",
    "    success_history.append(success)\n",
    "    epsilon_history.append(epsilon)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        avg_reward = np.mean(reward_history[-10:])\n",
    "        success_rate = np.mean(success_history[-10:])\n",
    "        print(f\"Episode {episode:3d}/{episodes} — Avg Reward: {avg_reward:.2f} — Success Rate: {success_rate:.2f} — epsilon: {epsilon:.3f}\")\n",
    "\n",
    "    if success:\n",
    "        if step_count <= 10:\n",
    "            quality = \"EXCELLENT\"\n",
    "        elif step_count <= 20:\n",
    "            quality = \"BON\"\n",
    "        else:\n",
    "            quality = \"NUL\"\n",
    "        print(f\"Episode {episode} : VICTOIRE ({quality}) en {step_count} pas ! Reward : {episode_reward:.2f}\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"Episode {episode} — FAIL en {step_count} pas — Reward: {episode_reward:.2f} — epsilon: {epsilon:.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f39f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.plot(reward_history)\n",
    "plt.axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "plt.title('Reward par épisode')\n",
    "plt.xlabel('Épisode')\n",
    "plt.ylabel('Reward')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "window = 25\n",
    "if len(reward_history) >= window:\n",
    "    moving_avg = [np.mean(reward_history[max(0,i-window):i+1]) for i in range(len(reward_history))]\n",
    "    plt.plot(moving_avg)\n",
    "    plt.axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "plt.title(f'Moyenne mobile ({window} épisodes)')\n",
    "plt.xlabel('Épisode')\n",
    "plt.ylabel('Reward moyenne')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "success_rate = np.cumsum(success_history) / np.arange(1, len(success_history)+1)\n",
    "plt.plot(success_rate * 100)\n",
    "plt.axhline(y=50, color='g', linestyle='--', alpha=0.3, label='50%')\n",
    "plt.title('Taux de succès cumulé (%)')\n",
    "plt.xlabel('Épisode')\n",
    "plt.ylabel('Taux de succès')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.plot(epsilon_history)\n",
    "plt.title('Évolution d\\'epsilon')\n",
    "plt.xlabel('Épisode')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf71624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "success = False\n",
    "step_count = 0\n",
    "\n",
    "# Charger le meilleur modèle\n",
    "if os.path.exists(best_model_path):\n",
    "    q_model.load_weights(best_model_path)\n",
    "    target_model.load_weights(best_model_path)\n",
    "\n",
    "while not done and step_count < 100:\n",
    "    action = np.argmax(q_model.predict(state[np.newaxis], verbose=0)[0])\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "    if reward > 0:\n",
    "        success = True\n",
    "    env.render()\n",
    "\n",
    "if success:\n",
    "    if step_count <= 10:\n",
    "        quality = \"EXCELLENT\"\n",
    "    elif step_count <= 20:\n",
    "        quality = \"BON\"\n",
    "    else:\n",
    "        quality = \"NUL\"\n",
    "    print(f\"Évaluation : VICTOIRE ({quality}) en {step_count} pas ! Reward : {total_reward:.2f}\")\n",
    "else:\n",
    "    print(f\"Évaluation : FAIL en {step_count} pas — Reward : {total_reward:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
