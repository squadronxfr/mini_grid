{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d11ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import minigrid\n",
    "from minigrid.wrappers import FullyObsWrapper\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    " \n",
    "env = gym.make(\"MiniGrid-Empty-16x16-v0\")\n",
    "env = FullyObsWrapper(env)\n",
    " \n",
    "# Hyperparamètres\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.001\n",
    "episodes = 500\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "memory_size = 10000\n",
    " \n",
    "# Réseau Q\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    " \n",
    "# Préparation\n",
    "obs, info = env.reset()\n",
    "img_shape = obs['image'].shape\n",
    "input_dim = np.prod(img_shape)\n",
    "n_actions = env.action_space.n\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    " \n",
    "policy_net = DQN(input_dim, n_actions).to(device)\n",
    "target_net = DQN(input_dim, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "memory = deque(maxlen=memory_size)\n",
    " \n",
    "def preprocess(obs):\n",
    "    # Convertit l'image en float et normalise\n",
    "    return torch.tensor(obs['image'].flatten(), dtype=torch.float32, device=device).unsqueeze(0)\n",
    " \n",
    "def calculate_reward(next_obs, done):\n",
    "    if done:\n",
    "        return 1.0\n",
    "    else:\n",
    "        img = next_obs['image']\n",
    "        agent_pos = np.where(img == 10)\n",
    "        goal_pos = np.where(img == 8)\n",
    "        agent_pos = (agent_pos[0][0], agent_pos[1][0]) if len(agent_pos[0]) > 0 else (0, 0)\n",
    "        goal_pos = (goal_pos[0][0], goal_pos[1][0]) if len(goal_pos[0]) > 0 else (0, 0)\n",
    "        distance = np.linalg.norm(np.array(agent_pos) - np.array(goal_pos))\n",
    "        return -0.00001 * distance\n",
    " \n",
    "rewards = []\n",
    "losses = []\n",
    "success_rates = []\n",
    "window_size = 100\n",
    " \n",
    "for episode in range(episodes):\n",
    "    obs, info = env.reset()\n",
    "    state = preprocess(obs)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    success = 0\n",
    " \n",
    "    while not done:\n",
    "        # Choix de l'action (epsilon-greedy)\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = policy_net(state)\n",
    "                action = torch.argmax(q_values).item()\n",
    " \n",
    "        next_obs, _, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        reward = calculate_reward(next_obs, done)\n",
    "        next_state = preprocess(next_obs)\n",
    " \n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    " \n",
    "        # Apprentissage\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions, rewards_b, next_states, dones = zip(*batch)\n",
    "            states = torch.cat(states).float()\n",
    "            next_states = torch.cat(next_states).float()\n",
    "            actions = torch.tensor(actions, device=device)\n",
    "            rewards_b = torch.tensor(rewards_b, dtype=torch.float32, device=device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device)\n",
    " \n",
    "            q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
    "            with torch.no_grad():\n",
    "                next_q = target_net(next_states).max(1)[0]\n",
    "                target = rewards_b + gamma * next_q * (1 - dones)\n",
    "            loss = nn.MSELoss()(q_values, target)\n",
    "            losses.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    " \n",
    "        # Mise à jour du réseau cible\n",
    "        if episode % 10 == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    " \n",
    "    rewards.append(total_reward)\n",
    "    if done and total_reward > 0:\n",
    "        success = 1\n",
    "    success_rates.append(success)\n",
    "    epsilon = max(min_epsilon, epsilon * np.exp(-decay_rate * episode))\n",
    "    print(f\"Episode {episode+1}: Reward = {total_reward}\")\n",
    " \n",
    "env.close()\n",
    " \n",
    "# Visualisations\n",
    "plt.figure(figsize=(12, 8))\n",
    " \n",
    "# Courbe des récompenses\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Evolution des récompenses (DQN)')\n",
    " \n",
    "# Courbe de la moyenne des récompenses\n",
    "plt.subplot(2, 2, 2)\n",
    "moving_avg = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "plt.plot(moving_avg)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Moyenne des récompenses')\n",
    "plt.title(f'Moyenne des récompenses (fenêtre de {window_size} épisodes)')\n",
    " \n",
    "# Courbe des pertes\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Perte')\n",
    "plt.title('Evolution de la perte (DQN)')\n",
    " \n",
    "# Courbe du taux de succès\n",
    "plt.subplot(2, 2, 4)\n",
    "success_rate = np.convolve(success_rates, np.ones(window_size)/window_size, mode='valid')\n",
    "plt.plot(success_rate)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Taux de succès')\n",
    "plt.title(f'Taux de succès (fenêtre de {window_size} épisodes)')\n",
    " \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
