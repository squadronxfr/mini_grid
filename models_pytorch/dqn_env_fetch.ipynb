{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d8116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "from minigrid.wrappers import FlatObsWrapper\n",
    "import random\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    " \n",
    "env_name = \"MiniGrid-Fetch-8x8-N3-v0\"\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.85\n",
    "gamma = 0.99\n",
    "batch_size = 128\n",
    "memory_size = 100_000\n",
    "episodes = 500\n",
    "target_update_freq = 1\n",
    "train_updates = 5\n",
    "learning_rate = 0.003\n",
    " \n",
    "env = gym.make(env_name)\n",
    "env = FlatObsWrapper(env)\n",
    "state, info = env.reset()\n",
    "state_shape = len(state)\n",
    "action_shape = env.action_space.n\n",
    "print(f\"State shape: {state_shape}, Action shape: {action_shape}\")\n",
    " \n",
    "log_dir = \"runs/minigrid_\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    " \n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    " \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "q_model = QNetwork(state_shape, action_shape).to(device)\n",
    "target_model = QNetwork(state_shape, action_shape).to(device)\n",
    "target_model.load_state_dict(q_model.state_dict())\n",
    "target_model.eval()\n",
    " \n",
    "optimizer = optim.Adam(q_model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "memory = deque(maxlen=memory_size)\n",
    " \n",
    "def store_transition(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    " \n",
    "def sample_batch():\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    state, action, reward, next_state, done = zip(*batch)\n",
    "    return (\n",
    "        torch.tensor(state, dtype=torch.float32).to(device),\n",
    "        torch.tensor(action, dtype=torch.int64).to(device),\n",
    "        torch.tensor(reward, dtype=torch.float32).to(device),\n",
    "        torch.tensor(next_state, dtype=torch.float32).to(device),\n",
    "        torch.tensor(done, dtype=torch.bool).to(device)\n",
    "    )\n",
    " \n",
    "def epsilon_greedy_policy(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, action_shape - 1)\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    q_values = q_model(state_tensor)\n",
    "    return torch.argmax(q_values, dim=1).item()\n",
    " \n",
    "def train_step(global_step):\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "    states, actions, rewards, next_states, dones = sample_batch()\n",
    " \n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_model(next_states)\n",
    "        max_next_q_values, _ = next_q_values.max(dim=1)\n",
    "        target_q = rewards + gamma * max_next_q_values * (~dones)\n",
    " \n",
    "    current_q = q_model(states)\n",
    "    selected_q = current_q.gather(1, actions.unsqueeze(1)).squeeze()\n",
    " \n",
    "    loss = loss_fn(selected_q, target_q)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    " \n",
    "    writer.add_scalar(\"Loss/train\", loss.item(), global_step)\n",
    "    return loss.item()\n",
    " \n",
    "# Entraînement\n",
    "reward_history = []\n",
    "global_step = 0\n",
    " \n",
    "for episode in range(1, episodes + 1):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    " \n",
    "    while not done:\n",
    "        action = epsilon_greedy_policy(state, epsilon)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        store_transition(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    " \n",
    "        loss = train_step(global_step)\n",
    "        global_step += 1\n",
    " \n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    if episode % target_update_freq == 0:\n",
    "        target_model.load_state_dict(q_model.state_dict())\n",
    " \n",
    "    reward_history.append(episode_reward)\n",
    "    writer.add_scalar(\"Reward/episode\", episode_reward, episode)\n",
    "    print(f\"Episode {episode}/{episodes} — Reward: {episode_reward:.2f} — epsilon: {epsilon:.3f}\")\n",
    " \n",
    "# Evaluation finale\n",
    "state, info = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    " \n",
    "while not done:\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    action = torch.argmax(q_model(state_tensor), dim=1).item()\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    total_reward += reward\n",
    "    env.render()\n",
    " \n",
    "print(f\"Récompense d'évaluation : {total_reward:.2f}\")\n",
    "writer.add_scalar(\"Reward/final_evaluation\", total_reward)\n",
    " \n",
    "# Graphique matplotlib\n",
    "plt.plot(reward_history, label=\"Reward per Episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Reward Progression\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"reward_plot.png\")\n",
    "plt.show()\n",
    " \n",
    "writer.close()\n",
    " \n",
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
