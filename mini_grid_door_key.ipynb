{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-01T12:01:56.119355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from minigrid.wrappers import ImgObsWrapper, RGBImgPartialObsWrapper\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "env_name = \"MiniGrid-DoorKey-8x8-v0\"\n",
    "episodes = 150\n",
    "gamma = 0.99  # Facteur de discount\n",
    "clip_ratio = 0.2  # Ratio de clipping pour PPO\n",
    "learning_rate = 0.0003 # Taux d'apprentissage unifié\n",
    "train_epochs = 10  # Nombre d'époques d'entraînement par batch\n",
    "gae_lambda = 0.95  # Lambda pour le Generalized Advantage Estimation\n",
    "batch_size = 64\n",
    "model_path = 'ppo_minigrid_doorkey_model.weights.h5'\n",
    "\n",
    "# Coefficients pour la perte combinée\n",
    "value_coeff = 0.5\n",
    "entropy_coeff = 0.01\n",
    "\n",
    "# --- Environnement ---\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "env = RGBImgPartialObsWrapper(env)\n",
    "env = ImgObsWrapper(env)\n",
    "\n",
    "state_shape = env.observation_space.shape\n",
    "action_shape = env.action_space.n\n",
    "\n",
    "# --- TensorBoard ---\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = f\"logs/mini_grid_doorkey/{current_time}\"\n",
    "\n",
    "\n",
    "# --- Modèle Actor-Critic pour PPO ---\n",
    "def create_actor_critic_model():\n",
    "    \"\"\"Crée un modèle avec deux têtes : une pour l'acteur (politique) et une pour le critique (valeur).\"\"\"\n",
    "    input_layer = tf.keras.layers.Input(shape=state_shape)\n",
    "    normalized = tf.keras.layers.Lambda(lambda x: x / 255.0)(input_layer)\n",
    "\n",
    "    # Base convolutionnelle partagée\n",
    "    conv1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')(normalized)\n",
    "    conv2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(conv1)\n",
    "    flattened = tf.keras.layers.Flatten()(conv2)\n",
    "\n",
    "    # Couche dense partagée\n",
    "    shared_dense = tf.keras.layers.Dense(128, activation='relu')(flattened)\n",
    "\n",
    "    # Tête de l'Acteur (Politique)\n",
    "    policy_logits = tf.keras.layers.Dense(action_shape, name='policy_logits')(shared_dense)\n",
    "\n",
    "    # Tête du Critique (Valeur)\n",
    "    value_output = tf.keras.layers.Dense(1, name='value')(shared_dense)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=[policy_logits, value_output])\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- Agent PPO ---\n",
    "class PPOAgent:\n",
    "    def __init__(self, weights_file_path=None):\n",
    "        \"\"\"Initialise l'agent. Crée la structure du modèle, puis charge les poids si un fichier existe.\"\"\"\n",
    "        print(\"Création de l'architecture du modèle PPO.\")\n",
    "        self.model = create_actor_critic_model()\n",
    "\n",
    "        if weights_file_path and os.path.exists(weights_file_path):\n",
    "            print(f\"Chargement des poids du modèle depuis : {weights_file_path}\")\n",
    "            self.model.load_weights(weights_file_path)\n",
    "        else:\n",
    "            print(\"Initialisation avec de nouveaux poids.\")\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        self.summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Sélectionne une action en se basant sur la politique actuelle.\"\"\"\n",
    "        logits, _ = self.model(np.expand_dims(state, 0), training=False)\n",
    "        action = tf.random.categorical(logits, 1)[0, 0].numpy()\n",
    "        return action\n",
    "\n",
    "    def compute_advantages_and_returns(self, rewards, values, dones):\n",
    "        \"\"\"Calcule les avantages et les retours en utilisant GAE (Generalized Advantage Estimation).\"\"\"\n",
    "        advantages = np.zeros_like(rewards, dtype=np.float32)\n",
    "        last_advantage = 0\n",
    "\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "            last_advantage = delta + gamma * gae_lambda * (1 - dones[t]) * last_advantage\n",
    "            advantages[t] = last_advantage\n",
    "\n",
    "        returns = advantages + values[:-1]\n",
    "        return advantages, returns\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, states, actions, old_probs, advantages, returns):\n",
    "        \"\"\"Effectue une seule étape d'entraînement en utilisant une perte combinée.\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            policy_logits, values = self.model(states, training=True)\n",
    "            values = tf.squeeze(values)\n",
    "\n",
    "            action_masks = tf.one_hot(actions, action_shape)\n",
    "            log_probs = tf.reduce_sum(action_masks * tf.nn.log_softmax(policy_logits), axis=1)\n",
    "\n",
    "            ratio = tf.exp(log_probs - old_probs)\n",
    "            clipped_ratio = tf.clip_by_value(ratio, 1 - clip_ratio, 1 + clip_ratio)\n",
    "\n",
    "            policy_loss = -tf.reduce_mean(tf.minimum(ratio * advantages, clipped_ratio * advantages))\n",
    "            value_loss = tf.reduce_mean(tf.square(returns - values))\n",
    "            entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.nn.softmax(policy_logits), logits=policy_logits))\n",
    "\n",
    "            total_loss = policy_loss + value_coeff * value_loss - entropy_coeff * entropy\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return total_loss, policy_loss, value_loss, entropy\n",
    "\n",
    "    def train(self, states, actions, rewards, dones, values, episode):\n",
    "        \"\"\"Orchestre la boucle d'entraînement PPO et log les résultats.\"\"\"\n",
    "        advantages, returns = self.compute_advantages_and_returns(rewards, values, dones)\n",
    "\n",
    "        advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)\n",
    "\n",
    "        logits, _ = self.model(np.array(states[:-1]), training=False)\n",
    "        action_masks = tf.one_hot(actions, action_shape)\n",
    "        old_log_probs = tf.reduce_sum(action_masks * tf.nn.log_softmax(logits), axis=1)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((states[:-1], actions, old_log_probs, advantages, returns))\n",
    "        dataset = dataset.shuffle(buffer_size=len(states)).batch(batch_size)\n",
    "\n",
    "        total_loss_avg, policy_loss_avg, value_loss_avg, entropy_avg = [], [], [], []\n",
    "\n",
    "        for _ in range(train_epochs):\n",
    "            for batch in dataset:\n",
    "                s_batch, a_batch, op_batch, adv_batch, r_batch = batch\n",
    "                t_loss, p_loss, v_loss, ent = self.train_step(s_batch, a_batch, op_batch, adv_batch, r_batch)\n",
    "                total_loss_avg.append(t_loss)\n",
    "                policy_loss_avg.append(p_loss)\n",
    "                value_loss_avg.append(v_loss)\n",
    "                entropy_avg.append(ent)\n",
    "\n",
    "        # Log des pertes moyennes pour cet épisode sur TensorBoard\n",
    "        with self.summary_writer.as_default():\n",
    "            tf.summary.scalar('total_loss', np.mean(total_loss_avg), step=episode)\n",
    "            tf.summary.scalar('policy_loss', np.mean(policy_loss_avg), step=episode)\n",
    "            tf.summary.scalar('value_loss', np.mean(value_loss_avg), step=episode)\n",
    "            tf.summary.scalar('entropy', np.mean(entropy_avg), step=episode)\n",
    "\n",
    "\n",
    "# --- Boucle d'entraînement principale ---\n",
    "agent = PPOAgent(model_path)\n",
    "reward_history = []\n",
    "success_count = 0\n",
    "\n",
    "print(f\"\\nDémarrage de l'entraînement PPO sur {env_name}\")\n",
    "print(f\"Logs TensorBoard: tensorboard --logdir {log_dir}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    states, actions, rewards, dones, values = [], [], [], [], []\n",
    "\n",
    "    done = False\n",
    "    steps = 0\n",
    "    while not done and steps < 250:\n",
    "        action = agent.select_action(state)\n",
    "\n",
    "        _, value = agent.model(np.expand_dims(state, 0), training=False)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        values.append(tf.squeeze(value).numpy())\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "    _, last_value = agent.model(np.expand_dims(state, 0), training=False)\n",
    "    values.append(tf.squeeze(last_value).numpy())\n",
    "\n",
    "    states.append(state)\n",
    "\n",
    "    # Entraîner l'agent et passer le numéro de l'épisode pour les logs\n",
    "    agent.train(np.array(states), np.array(actions), np.array(rewards), np.array(dones), np.array(values), episode)\n",
    "\n",
    "    reward_history.append(episode_reward)\n",
    "    # Log immédiat en cas de victoire\n",
    "    if reward > 0.5:\n",
    "        success_count += 1\n",
    "        print(f\"  -> Episode {episode+1}: VICTOIRE! Reward: {episode_reward:.3f}, Pas: {steps}\")\n",
    "\n",
    "    # Log de résumé tous les 10 épisodes\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        last_10_rewards = reward_history[-10:]\n",
    "        avg_reward = np.mean(last_10_rewards)\n",
    "        successes_in_window = sum(1 for r in last_10_rewards if r > 0.5)\n",
    "\n",
    "        print(f\"Episode {episode+1}/{episodes} | Reward moyen (10 ep): {avg_reward:.2f} | Victoires (10 ep): {successes_in_window}/10\")\n",
    "\n",
    "# --- Fin de l'entraînement et résultats ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Entraînement terminé!\")\n",
    "agent.model.save_weights(model_path)\n",
    "print(f\"Poids du modèle sauvegardés dans: {model_path}\")\n",
    "print(f\"Total des victoires: {success_count}/{episodes} ({100*success_count/episodes:.1f}%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Affichage des graphiques\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(reward_history)\n",
    "plt.title('Reward par Épisode')\n",
    "plt.xlabel('Épisode')\n",
    "plt.ylabel('Reward Total')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Test final du modèle\n",
    "print(\"\\nTest final du modèle (5 tentatives)\")\n",
    "print(\"-\" * 40)\n",
    "test_successes = 0\n",
    "for i in range(5):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    while not done and steps < 250:\n",
    "        action = agent.select_action(state)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        steps += 1\n",
    "    if reward > 0.5:\n",
    "        test_successes += 1\n",
    "        print(f\"Test {i+1}: VICTOIRE en {steps} pas!\")\n",
    "    else:\n",
    "        print(f\"Test {i+1}: Échec après {steps} pas\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Score final: {test_successes}/5 victoires\")\n",
    "\n",
    "env.close()\n"
   ],
   "id": "c78bea35db08eac2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création de l'architecture du modèle PPO.\n",
      "Chargement des poids du modèle depuis : ppo_minigrid_doorkey_model.weights.h5\n",
      "\n",
      "Démarrage de l'entraînement PPO sur MiniGrid-DoorKey-8x8-v0\n",
      "Logs TensorBoard: tensorboard --logdir logs/mini_grid_doorkey/20250801-140156\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e866d3022c068dad",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
